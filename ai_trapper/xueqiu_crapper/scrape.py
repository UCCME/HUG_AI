import argparse
import json
import os
import time
from pathlib import Path
from typing import Dict, List, Optional
import random

import requests
from dotenv import load_dotenv

"""
é›?ç?å?ä¸?çˆ?å?è„?æ‰?æ?è¯´æ˜ï¼? - éœ€è¦æœ‰æ•ˆçš„é›ªçƒ Cookieï¼ˆè‡³å°‘åŒ…å?xq_a_token / xq_r_tokenï¼‰ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡æˆ?.env æ–‡ä»¶æä¾›ã€? - æœ¬è„šæœ¬æœªå†…ç½®è´¦å·ç™»å½•ï¼Œè¯·å…ˆåœ¨æµè§ˆå™¨ç™»å½•é›ªçƒåæ‹·è´ Cookieã€? - å½“å‰ç¯å¢ƒç½‘ç»œå—é™ï¼Œéœ€åœ¨å¯è”ç½‘ç¯å¢ƒä¸‹è¿è¡Œã€?"""

API_URL = "https://xueqiu.com/v4/statuses/user_timeline.json"
# æ›´çœŸå®çš„æµè§ˆå™¨User-Agentåˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0",
]

DEFAULT_HEADERS = {
    "Accept": "application/json, text/plain, */*",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "Host": "xueqiu.com",
    "Pragma": "no-cache",
    "Referer": "https://xueqiu.com/",
    "Sec-Ch-Ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
    "Sec-Ch-Ua-Mobile": "?0",
    "Sec-Ch-Ua-Platform": '"Windows"',
    "Sec-Fetch-Dest": "empty",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-origin",
    "X-Requested-With": "XMLHttpRequest",
}


def load_cookie() -> str:
    load_dotenv()
    cookie = os.getenv("XUEQIU_COOKIE", "").strip()
    if not cookie:
        raise ValueError("è¯·åœ¨ç¯å¢ƒå˜é‡æˆ?.env ä¸­è®¾ç½?XUEQIU_COOKIE")
    return cookie


def get_random_headers() -> Dict[str, str]:
    """ç”ŸæˆåŒ…å«éšæœºUser-Agentå’Œå¯èƒ½å˜åŒ–çš„å…¶ä»–å¤´éƒ¨çš„è¯·æ±‚å¤´"""
    headers = DEFAULT_HEADERS.copy()
    headers["User-Agent"] = random.choice(USER_AGENTS)
    # å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€è°ƒæ•?Sec-Ch-Ua-Platform ç­‰å­—æ®?    if "Mac" in headers["User-Agent"]:
        headers["Sec-Ch-Ua-Platform"] = '"macOS"'
    return headers

def load_proxies() -> Optional[List[str]]:
    """ä»ç¯å¢ƒå˜é‡åŠ è½½ä»£ç†åˆ—è¡?""
    proxy_string = os.getenv("PROXY_POOL")
    if proxy_string:
        return [p.strip() for p in proxy_string.split(",") if p.strip()]
    return None

def fetch_page(user_id: str, page: int, count: int, cookie: str, timeout: int = 30) -> Dict:
    params = {
        "user_id": user_id,
        "page": page,
        "count": count,
    }
    
    # ä½¿ç”¨éšæœºUser-Agent
    headers = DEFAULT_HEADERS.copy()
    headers["User-Agent"] = random.choice(USER_AGENTS)
    headers["Cookie"] = cookie
    
    # å¢åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸?(3-8ç§?
    time.sleep(random.uniform(3, 8))
    
    try:
        resp = requests.get(API_URL, params=params, headers=headers, timeout=timeout)
        resp.raise_for_status()
        
        # æ£€æŸ¥è¿”å›å†…å®¹æ˜¯å¦åŒ…å«WAFæ‹¦æˆªç‰¹å¾
        if "aliyun_waf" in resp.text or "_waf_" in resp.text or "<textarea id=\"renderData\"" in resp.text:
            raise ValueError(
                f"è¯·æ±‚è¢«é˜¿é‡Œäº‘WAFé˜²ç«å¢™æ‹¦æˆªï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieæˆ–é™ä½è®¿é—®é¢‘ç‡ï¼Œstatus={resp.status_code}"
            )
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç™»å½•é¡µé¢
        if "ç™»å½•" in resp.text and "xueqiu" in resp.text and "user" not in resp.text:
            raise ValueError(
                f"è¿”å›å†…å®¹ä¼¼ä¹æ˜¯ç™»å½•é¡µé¢ï¼Œå¯èƒ½ Cookie å¤±æ•ˆ/ä¸å®Œæ•´ï¼Œstatus={resp.status_code}"
            )
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºç™½é¡µé¢æˆ–è€…å…¶ä»–é”™è¯¯é¡µé?        if not resp.text.strip() or len(resp.text.strip()) < 20:
            raise ValueError(
                f"è¿”å›å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯è¯·æ±‚è¢«æ‹¦æˆªï¼Œstatus={resp.status_code}, body_length={len(resp.text)}"
            )
        
        return resp.json()
    except requests.RequestException as e:
        raise ValueError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}") from e
    except Exception as exc:  # å¤„ç†éJSONè¿”å›ï¼ˆå¦‚æœªæˆæƒã€è·³ç™»å½•é¡µç­‰ï¼?        raise ValueError(
            f"æ¥å£è¿”å›éJSONï¼Œå¯èƒ?Cookie å¤±æ•ˆ/ä¸å®Œæ•´ï¼Œstatus={resp.status_code if 'resp' in locals() else 'unknown'}, "
            f"body={resp.text[:500] if 'resp' in locals() and hasattr(resp, 'text') else 'unknown'}"
        ) from exc


def parse_item(raw: Dict) -> Dict:
    # ä»…æå–å¸¸ç”¨å­—æ®µï¼Œå¯æŒ‰éœ€æ‰©å±•
    return {
        "id": raw.get("id"),
        "title": raw.get("title"),
        "text": raw.get("text"),
        "created_at": raw.get("created_at"),
        "retweet_count": raw.get("retweet_count"),
        "reply_count": raw.get("reply_count"),
        "like_count": raw.get("like_count"),
        "view_count": raw.get("view_count"),
    }


def save_records(records: List[Dict], out_path: Path, fmt: str):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    if fmt == "jsonl":
        with out_path.open("w", encoding="utf-8") as f:
            for r in records:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
    elif fmt == "csv":
        import csv

        if not records:
            return
        with out_path.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=list(records[0].keys()))
            writer.writeheader()
            writer.writerows(records)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {fmt}")


def crawl_user(
    user_id: str,
    pages: int = 5,
    count: int = 20,
    delay: float = 5.0,  # é»˜è®¤å»¶è¿Ÿå¢åŠ åˆ?ç§?    fmt: str = "jsonl",
    outdir: str = "output",
    keyword: Optional[str] = None,
):
    cookie = load_cookie()
    all_items: List[Dict] = []
    
    for page in range(1, pages + 1):
        retry_count = 0
        max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
        
        while retry_count < max_retries:
            try:
                print(f"æ­£åœ¨è·å–ç¬?{page} é¡µæ•°æ?..")
                data = fetch_page(user_id, page, count, cookie)
                break  # æˆåŠŸè·å–åˆ™è·³å‡ºé‡è¯•å¾ªç?            except ValueError as e:
                retry_count += 1
                print(f"ç¬?{page} é¡µè·å–å¤±è´¥ï¼Œç¬?{retry_count} æ¬¡é‡è¯? {str(e)}")
                
                if "WAFé˜²ç«å¢™æ‹¦æˆ? in str(e):
                    # å¦‚æœæ˜¯WAFæ‹¦æˆªï¼Œå¢åŠ æ›´é•¿çš„å»¶è¿Ÿ
                    sleep_time = delay * retry_count * 2
                    print(f"WAFæ‹¦æˆªï¼Œå¢åŠ å»¶è¿?{sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                elif retry_count >= max_retries:
                    raise e  # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºå¼‚å¸¸
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œæ­£å¸¸å»¶è¿Ÿåé‡è¯•
                    sleep_time = delay * retry_count
                    print(f"å°†åœ¨ {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
            except Exception as e:
                retry_count += 1
                print(f"ç¬?{page} é¡µå‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œç¬?{retry_count} æ¬¡é‡è¯? {str(e)}")
                if retry_count >= max_retries:
                    raise e
                time.sleep(delay * retry_count)
        
        items = data.get("statuses", [])
        parsed = [parse_item(x) for x in items]
        if keyword:
            parsed = [p for p in parsed if p.get("text") and keyword in p.get("text")]
        all_items.extend(parsed)
        print(f"ç¬?{page} é¡? è·å–åˆ?{len(items)} æ¡æ•°æ®ï¼Œç­›é€‰åä¿ç•™ {len(parsed)} æ?)
        
        # é¡µé¢é—´å¢åŠ éšæœºå»¶è¿?        page_delay = random.uniform(delay, delay * 2)
        print(f"é¡µé¢é—´å»¶è¿?{page_delay:.1f} ç§?..")
        time.sleep(page_delay)

    out_path = Path(outdir) / f"user_{user_id}.{fmt}"
    save_records(all_items, out_path, fmt)
    print(f"ä¿å­˜å®Œæˆ: {out_path} ({len(all_items)} æ?")


def main():
    parser = argparse.ArgumentParser(description="é›ªçƒåšä¸»çˆ¬å–è„šæœ¬")
    parser.add_argument("--user", required=True, help="é›ªçƒç”¨æˆ·ID")
    parser.add_argument("--pages", type=int, default=5, help="æŠ“å–é¡µæ•°")
    parser.add_argument("--count", type=int, default=20, help="æ¯é¡µæ¡æ•°")
    parser.add_argument("--delay", type=float, default=5.0, help="ä¸¤é¡µä¹‹é—´çš„åŸºç¡€å»¶æ—¶(ç§?")  # æ›´æ–°é»˜è®¤å€?    parser.add_argument("--proxy-pool", type=str, help="é€—å·åˆ†éš”çš„HTTPSä»£ç†åœ°å€åˆ—è¡¨ï¼Œä¾‹å¦? https://ip:port,https://ip2:port")
    parser.add_argument("--format", choices=["jsonl", "csv"], default="jsonl", help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--outdir", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--keyword", default=None, help="ä»…ä¿ç•™åŒ…å«è¯¥å…³é”®å­—çš„å¸–å­")
    args = parser.parse_args()

    # å¦‚æœå‘½ä»¤è¡Œæä¾›äº†ä»£ç†æ± ï¼Œåˆ™è®¾ç½®ç¯å¢ƒå˜é‡?    if args.proxy_pool:
        os.environ["PROXY_POOL"] = args.proxy_pool
        
    crawl_user(
        user_id=args.user,
        pages=args.pages,
        count=args.count,
        delay=args.delay,
        fmt=args.format,
        outdir=args.outdir,
        keyword=args.keyword,
    )


if __name__ == "__main__":
    main()

